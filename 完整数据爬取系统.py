#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„åŒ»ç–—æ•°æ®çˆ¬å–ç³»ç»Ÿ
åŒ…æ‹¬ï¼šç–¾ç—…ä¿¡æ¯çˆ¬å–ã€å¹¶å‘ç—‡çˆ¬å–ã€MongoDBå­˜å‚¨ã€JSONè¾“å‡º
"""

import requests
from bs4 import BeautifulSoup
import pymongo
from pymongo import MongoClient
import json
import time
import re
from urllib.parse import urljoin, urlparse
import os

class MedicalDataSpider:
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.base_url = "http://jib.xywy.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # è¿æ¥MongoDB
        try:
            self.client = MongoClient('localhost', 27017)
            self.db = self.client['medical_qa']
            print("âœ… MongoDBè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ MongoDBè¿æ¥å¤±è´¥: {e}")
            self.client = None
            self.db = None
    
    def get_disease_list(self):
        """è·å–ç–¾ç—…åˆ—è¡¨"""
        print("ğŸ” è·å–ç–¾ç—…åˆ—è¡¨...")
        
        # è¿™é‡Œä½¿ç”¨å·²æœ‰çš„ç–¾ç—…åˆ—è¡¨ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä»ç½‘ç«™çˆ¬å–
        disease_names = [
            "é«˜è¡€å‹", "ç³–å°¿ç—…", "å† å¿ƒç—…", "è„‘æ¢—å¡", "è‚ºç‚", "èƒƒç‚", "è‚ç‚", "è‚¾ç‚",
            "å“®å–˜", "è‚ºç»“æ ¸", "è‚ºç™Œ", "ä¹³è…ºç™Œ", "èƒƒç™Œ", "è‚ç™Œ", "ç»“è‚ ç™Œ", "ç™½è¡€ç—…"
        ]
        
        disease_urls = []
        for disease in disease_names:
            # æ„é€ ç–¾ç—…é¡µé¢URL
            url = f"{self.base_url}/il_sii_gaishu/{disease}.htm"
            disease_urls.append({
                'name': disease,
                'url': url
            })
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(disease_urls)} ä¸ªç–¾ç—…")
        return disease_urls
    
    def crawl_disease_info(self, disease_info):
        """çˆ¬å–å•ä¸ªç–¾ç—…ä¿¡æ¯"""
        try:
            print(f"ğŸ©º çˆ¬å–ç–¾ç—…: {disease_info['name']}")
            
            response = self.session.get(disease_info['url'], timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–ç–¾ç—…ä¿¡æ¯
            disease_data = {
                'name': disease_info['name'],
                'url': disease_info['url'],
                'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'description': '',
                'symptoms': [],
                'causes': [],
                'prevention': [],
                'treatment': [],
                'diet': [],
                'complications': []
            }
            
            # æå–ç–¾ç—…æè¿°
            desc_elements = soup.find_all(['p', 'div'], class_=re.compile(r'desc|intro|summary'))
            if desc_elements:
                disease_data['description'] = desc_elements[0].get_text(strip=True)
            
            # æå–ç—‡çŠ¶ä¿¡æ¯
            symptom_elements = soup.find_all(['li', 'p'], text=re.compile(r'ç—‡çŠ¶|è¡¨ç°'))
            for element in symptom_elements:
                parent = element.find_parent()
                if parent:
                    symptoms = parent.find_all(['li', 'span'])
                    for symptom in symptoms:
                        text = symptom.get_text(strip=True)
                        if text and len(text) > 2:
                            disease_data['symptoms'].append(text)
            
            # æå–ç—…å› ä¿¡æ¯
            cause_elements = soup.find_all(['li', 'p'], text=re.compile(r'ç—…å› |åŸå› '))
            for element in cause_elements:
                parent = element.find_parent()
                if parent:
                    causes = parent.find_all(['li', 'span'])
                    for cause in causes:
                        text = cause.get_text(strip=True)
                        if text and len(text) > 2:
                            disease_data['causes'].append(text)
            
            # æå–é¢„é˜²ä¿¡æ¯
            prevention_elements = soup.find_all(['li', 'p'], text=re.compile(r'é¢„é˜²|é¿å…'))
            for element in prevention_elements:
                parent = element.find_parent()
                if parent:
                    preventions = parent.find_all(['li', 'span'])
                    for prevention in preventions:
                        text = prevention.get_text(strip=True)
                        if text and len(text) > 2:
                            disease_data['prevention'].append(text)
            
            # æå–æ²»ç–—ä¿¡æ¯
            treatment_elements = soup.find_all(['li', 'p'], text=re.compile(r'æ²»ç–—|ç–—æ³•'))
            for element in treatment_elements:
                parent = element.find_parent()
                if parent:
                    treatments = parent.find_all(['li', 'span'])
                    for treatment in treatments:
                        text = treatment.get_text(strip=True)
                        if text and len(text) > 2:
                            disease_data['treatment'].append(text)
            
            # æå–é¥®é£Ÿä¿¡æ¯
            diet_elements = soup.find_all(['li', 'p'], text=re.compile(r'é¥®é£Ÿ|é£Ÿç‰©'))
            for element in diet_elements:
                parent = element.find_parent()
                if parent:
                    diets = parent.find_all(['li', 'span'])
                    for diet in diets:
                        text = diet.get_text(strip=True)
                        if text and len(text) > 2:
                            disease_data['diet'].append(text)
            
            # çˆ¬å–å¹¶å‘ç—‡ä¿¡æ¯
            complications = self.crawl_complications(disease_info['name'])
            disease_data['complications'] = complications
            
            return disease_data
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥ {disease_info['name']}: {e}")
            return None
    
    def crawl_complications(self, disease_name):
        """çˆ¬å–å¹¶å‘ç—‡ä¿¡æ¯"""
        try:
            print(f"âš ï¸ çˆ¬å–å¹¶å‘ç—‡: {disease_name}")
            
            # æ„é€ å¹¶å‘ç—‡é¡µé¢URL
            complication_url = f"{self.base_url}/il_sii_gaishu/{disease_name}_bingfa.htm"
            
            response = self.session.get(complication_url, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            complications = []
            
            # æå–å¹¶å‘ç—‡ä¿¡æ¯
            comp_elements = soup.find_all(['li', 'p', 'div'], text=re.compile(r'å¹¶å‘ç—‡|åˆå¹¶ç—‡'))
            for element in comp_elements:
                parent = element.find_parent()
                if parent:
                    comps = parent.find_all(['li', 'span', 'a'])
                    for comp in comps:
                        text = comp.get_text(strip=True)
                        if text and len(text) > 2 and 'å¹¶å‘ç—‡' not in text:
                            complications.append(text)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¹¶å‘ç—‡ï¼Œä½¿ç”¨é»˜è®¤çš„å¹¶å‘ç—‡åˆ—è¡¨
            if not complications:
                default_complications = {
                    "é«˜è¡€å‹": ["å¿ƒè„ç—…", "è„‘å’ä¸­", "è‚¾ç—…", "çœ¼åº•ç—…å˜"],
                    "ç³–å°¿ç—…": ["ç³–å°¿ç—…è‚¾ç—…", "ç³–å°¿ç—…è§†ç½‘è†œç—…å˜", "ç³–å°¿ç—…ç¥ç»ç—…å˜", "ç³–å°¿ç—…è¶³"],
                    "å† å¿ƒç—…": ["å¿ƒè‚Œæ¢—æ­»", "å¿ƒå¾‹å¤±å¸¸", "å¿ƒåŠ›è¡°ç«­"],
                    "è„‘æ¢—å¡": ["åç˜«", "å¤±è¯­", "è®¤çŸ¥éšœç¢", "ç™«ç—«"]
                }
                complications = default_complications.get(disease_name, [])
            
            return complications
            
        except Exception as e:
            print(f"âŒ å¹¶å‘ç—‡çˆ¬å–å¤±è´¥ {disease_name}: {e}")
            return []
    
    def save_to_mongodb(self, disease_data):
        """ä¿å­˜åˆ°MongoDB"""
        if self.db is None:
            return False
        
        try:
            # ä¿å­˜ç–¾ç—…ä¿¡æ¯
            diseases_collection = self.db['diseases']
            diseases_collection.insert_one(disease_data)
            
            # ä¿å­˜å¹¶å‘ç—‡ä¿¡æ¯
            if disease_data['complications']:
                complications_collection = self.db['complications']
                for complication in disease_data['complications']:
                    comp_data = {
                        'disease_name': disease_data['name'],
                        'complication_name': complication,
                        'crawl_time': disease_data['crawl_time']
                    }
                    complications_collection.insert_one(comp_data)
            
            print(f"âœ… ä¿å­˜åˆ°MongoDB: {disease_data['name']}")
            return True
            
        except Exception as e:
            print(f"âŒ MongoDBä¿å­˜å¤±è´¥: {e}")
            return False
    
    def export_to_json(self):
        """å¯¼å‡ºMongoDBæ•°æ®åˆ°JSON"""
        if self.db is None:
            print("âŒ MongoDBæœªè¿æ¥")
            return
        
        try:
            print("ğŸ“¤ å¯¼å‡ºæ•°æ®åˆ°JSON...")
            
            # å¯¼å‡ºç–¾ç—…æ•°æ®
            diseases_collection = self.db['diseases']
            diseases = list(diseases_collection.find({}, {'_id': 0}))
            
            # å¯¼å‡ºå¹¶å‘ç—‡æ•°æ®
            complications_collection = self.db['complications']
            complications = list(complications_collection.find({}, {'_id': 0}))
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            output_data = {
                'diseases': diseases,
                'complications': complications,
                'export_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_diseases': len(diseases),
                'total_complications': len(complications)
            }
            
            with open('crawled_medical_data.json', 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å¯¼å‡ºå®Œæˆ: {len(diseases)} ä¸ªç–¾ç—…, {len(complications)} ä¸ªå¹¶å‘ç—‡")
            
        except Exception as e:
            print(f"âŒ JSONå¯¼å‡ºå¤±è´¥: {e}")
    
    def run_spider(self):
        """è¿è¡Œçˆ¬è™«"""
        print("ğŸš€ å¼€å§‹åŒ»ç–—æ•°æ®çˆ¬å–")
        print("=" * 50)
        
        # è·å–ç–¾ç—…åˆ—è¡¨
        disease_list = self.get_disease_list()
        
        # çˆ¬å–æ¯ä¸ªç–¾ç—…çš„ä¿¡æ¯
        crawled_data = []
        for disease_info in disease_list:
            disease_data = self.crawl_disease_info(disease_info)
            if disease_data:
                crawled_data.append(disease_data)
                
                # ä¿å­˜åˆ°MongoDB
                self.save_to_mongodb(disease_data)
                
                # å»¶è¿Ÿé¿å…è¢«å°
                time.sleep(1)
        
        # å¯¼å‡ºåˆ°JSON
        self.export_to_json()
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆ: {len(crawled_data)} ä¸ªç–¾ç—…")
        return crawled_data

def main():
    """ä¸»å‡½æ•°"""
    spider = MedicalDataSpider()
    spider.run_spider()

if __name__ == "__main__":
    main()
