# åŒ»ç–—æ•°æ®çˆ¬å–è¯¦ç»†æŒ‡å—

## ğŸ¯ çˆ¬å–ç›®æ ‡

### æ•°æ®æ¥æº
- **ç½‘ç«™**ï¼šå¯»åŒ»é—®è¯ç½‘ (http://jib.xywy.com/)
- **æ•°æ®ç±»å‹**ï¼šç–¾ç—…ä¿¡æ¯ã€ç—‡çŠ¶ã€ç—…å› ã€æ²»ç–—ã€å¹¶å‘ç—‡ç­‰
- **å­˜å‚¨æ–¹å¼**ï¼šMongoDB + JSONæ–‡ä»¶

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–åŒ…
```bash
pip3 install requests beautifulsoup4 pymongo
```

### 2. å¯åŠ¨MongoDBï¼ˆå¯é€‰ï¼‰
```bash
# å¦‚æœä½¿ç”¨MongoDBå­˜å‚¨
mongod --dbpath /path/to/your/db
```

### 3. è¿è¡Œçˆ¬è™«
```bash
python3 å®Œæ•´æ•°æ®çˆ¬å–ç³»ç»Ÿ.py
```

## ğŸ“Š çˆ¬å–æ•°æ®è¯¦æƒ…

### 1. ç–¾ç—…ä¿¡æ¯çˆ¬å–

#### ç›®æ ‡é¡µé¢ç»“æ„
```
http://jib.xywy.com/il_sii_gaishu/{ç–¾ç—…åç§°}.htm
```

#### çˆ¬å–å­—æ®µ
- **ç–¾ç—…åç§°**ï¼šç–¾ç—…çš„ä¸­æ–‡åç§°
- **ç–¾ç—…æè¿°**ï¼šç–¾ç—…çš„è¯¦ç»†æè¿°
- **ç—‡çŠ¶ä¿¡æ¯**ï¼šç–¾ç—…çš„ä¸»è¦ç—‡çŠ¶
- **ç—…å› åˆ†æ**ï¼šç–¾ç—…çš„åŸå› åˆ†æ
- **é¢„é˜²æªæ–½**ï¼šç–¾ç—…çš„é¢„é˜²æ–¹æ³•
- **æ²»ç–—æ–¹æ³•**ï¼šç–¾ç—…çš„æ²»ç–—æ–¹å¼
- **é¥®é£Ÿå»ºè®®**ï¼šç›¸å…³çš„é¥®é£ŸæŒ‡å¯¼
- **å¹¶å‘ç—‡**ï¼šç–¾ç—…çš„å¹¶å‘ç—‡ä¿¡æ¯

### 2. å¹¶å‘ç—‡ä¿¡æ¯çˆ¬å–

#### ç›®æ ‡é¡µé¢ç»“æ„
```
http://jib.xywy.com/il_sii_gaishu/{ç–¾ç—…åç§°}_bingfa.htm
```

#### çˆ¬å–å†…å®¹
- **å¹¶å‘ç—‡åç§°**ï¼šå¹¶å‘ç—‡çš„ä¸­æ–‡åç§°
- **å¹¶å‘ç—‡æè¿°**ï¼šå¹¶å‘ç—‡çš„è¯¦ç»†æè¿°
- **å…³è”ç–¾ç—…**ï¼šä¸åŸç–¾ç—…çš„å…³ç³»
- **ä¸¥é‡ç¨‹åº¦**ï¼šå¹¶å‘ç—‡çš„ä¸¥é‡ç¨‹åº¦

## ğŸ”§ çˆ¬è™«æŠ€æœ¯å®ç°

### 1. åŸºç¡€çˆ¬è™«ç±»

```python
class MedicalDataSpider:
    def __init__(self):
        self.base_url = "http://jib.xywy.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
```

### 2. æ•°æ®æå–æ–¹æ³•

#### HTMLè§£æ
```python
def parse_disease_page(self, html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æå–ç–¾ç—…æè¿°
    desc_elements = soup.find_all(['p', 'div'], class_=re.compile(r'desc|intro'))
    
    # æå–ç—‡çŠ¶ä¿¡æ¯
    symptom_elements = soup.find_all(['li', 'p'], text=re.compile(r'ç—‡çŠ¶'))
    
    # æå–ç—…å› ä¿¡æ¯
    cause_elements = soup.find_all(['li', 'p'], text=re.compile(r'ç—…å› '))
    
    return extracted_data
```

#### æ•°æ®æ¸…æ´—
```python
def clean_data(self, raw_data):
    # å»é™¤HTMLæ ‡ç­¾
    cleaned_text = re.sub(r'<[^>]+>', '', raw_data)
    
    # å»é™¤å¤šä½™ç©ºç™½
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    
    # å»é™¤ç‰¹æ®Šå­—ç¬¦
    cleaned_text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', cleaned_text)
    
    return cleaned_text
```

### 3. æ•°æ®å­˜å‚¨

#### MongoDBå­˜å‚¨
```python
def save_to_mongodb(self, disease_data):
    try:
        client = MongoClient('localhost', 27017)
        db = client['medical_qa']
        
        # ä¿å­˜ç–¾ç—…ä¿¡æ¯
        diseases_collection = db['diseases']
        diseases_collection.insert_one(disease_data)
        
        # ä¿å­˜å¹¶å‘ç—‡ä¿¡æ¯
        if disease_data['complications']:
            complications_collection = db['complications']
            for complication in disease_data['complications']:
                comp_data = {
                    'disease_name': disease_data['name'],
                    'complication_name': complication,
                    'crawl_time': disease_data['crawl_time']
                }
                complications_collection.insert_one(comp_data)
        
        print(f"âœ… ä¿å­˜åˆ°MongoDB: {disease_data['name']}")
        return True
        
    except Exception as e:
        print(f"âŒ MongoDBä¿å­˜å¤±è´¥: {e}")
        return False
```

#### JSONæ–‡ä»¶å­˜å‚¨
```python
def export_to_json(self):
    try:
        # ä»MongoDBè¯»å–æ•°æ®
        diseases = list(self.db['diseases'].find({}, {'_id': 0}))
        complications = list(self.db['complications'].find({}, {'_id': 0}))
        
        # ç»„ç»‡æ•°æ®
        output_data = {
            'diseases': diseases,
            'complications': complications,
            'export_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_diseases': len(diseases),
            'total_complications': len(complications)
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open('crawled_medical_data.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å¯¼å‡ºå®Œæˆ: {len(diseases)} ä¸ªç–¾ç—…, {len(complications)} ä¸ªå¹¶å‘ç—‡")
        
    except Exception as e:
        print(f"âŒ JSONå¯¼å‡ºå¤±è´¥: {e}")
```

## ğŸ“‹ çˆ¬å–æµç¨‹è¯¦è§£

### 1. æ•°æ®è·å–æµç¨‹

```
å¼€å§‹ â†’ è·å–ç–¾ç—…åˆ—è¡¨ â†’ éå†ç–¾ç—… â†’ çˆ¬å–ç–¾ç—…ä¿¡æ¯ â†’ çˆ¬å–å¹¶å‘ç—‡ â†’ æ•°æ®æ¸…æ´— â†’ æ•°æ®å­˜å‚¨ â†’ å¯¼å‡ºJSON â†’ ç»“æŸ
```

### 2. è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤1ï¼šè·å–ç–¾ç—…åˆ—è¡¨
```python
def get_disease_list(self):
    # ä»é¢„å®šä¹‰åˆ—è¡¨æˆ–ç½‘ç«™è·å–ç–¾ç—…åç§°
    disease_names = [
        "é«˜è¡€å‹", "ç³–å°¿ç—…", "å† å¿ƒç—…", "è„‘æ¢—å¡", "è‚ºç‚", "èƒƒç‚", 
        "è‚ç‚", "è‚¾ç‚", "å“®å–˜", "è‚ºç»“æ ¸", "è‚ºç™Œ", "ä¹³è…ºç™Œ"
    ]
    
    disease_urls = []
    for disease in disease_names:
        url = f"{self.base_url}/il_sii_gaishu/{disease}.htm"
        disease_urls.append({'name': disease, 'url': url})
    
    return disease_urls
```

#### æ­¥éª¤2ï¼šçˆ¬å–å•ä¸ªç–¾ç—…
```python
def crawl_disease_info(self, disease_info):
    try:
        # å‘é€HTTPè¯·æ±‚
        response = self.session.get(disease_info['url'], timeout=10)
        response.encoding = 'utf-8'
        
        # è§£æHTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ•°æ®
        disease_data = self.extract_disease_data(soup, disease_info)
        
        return disease_data
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥ {disease_info['name']}: {e}")
        return None
```

#### æ­¥éª¤3ï¼šæ•°æ®æå–
```python
def extract_disease_data(self, soup, disease_info):
    disease_data = {
        'name': disease_info['name'],
        'url': disease_info['url'],
        'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'description': '',
        'symptoms': [],
        'causes': [],
        'prevention': [],
        'treatment': [],
        'diet': [],
        'complications': []
    }
    
    # æå–ç–¾ç—…æè¿°
    desc_elements = soup.find_all(['p', 'div'], class_=re.compile(r'desc|intro|summary'))
    if desc_elements:
        disease_data['description'] = desc_elements[0].get_text(strip=True)
    
    # æå–ç—‡çŠ¶ä¿¡æ¯
    symptom_elements = soup.find_all(['li', 'p'], text=re.compile(r'ç—‡çŠ¶|è¡¨ç°'))
    for element in symptom_elements:
        parent = element.find_parent()
        if parent:
            symptoms = parent.find_all(['li', 'span'])
            for symptom in symptoms:
                text = symptom.get_text(strip=True)
                if text and len(text) > 2:
                    disease_data['symptoms'].append(text)
    
    # æå–å…¶ä»–ä¿¡æ¯...
    
    return disease_data
```

#### æ­¥éª¤4ï¼šå¹¶å‘ç—‡çˆ¬å–
```python
def crawl_complications(self, disease_name):
    try:
        # æ„é€ å¹¶å‘ç—‡é¡µé¢URL
        complication_url = f"{self.base_url}/il_sii_gaishu/{disease_name}_bingfa.htm"
        
        response = self.session.get(complication_url, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        complications = []
        
        # æå–å¹¶å‘ç—‡ä¿¡æ¯
        comp_elements = soup.find_all(['li', 'p', 'div'], text=re.compile(r'å¹¶å‘ç—‡|åˆå¹¶ç—‡'))
        for element in comp_elements:
            parent = element.find_parent()
            if parent:
                comps = parent.find_all(['li', 'span', 'a'])
                for comp in comps:
                    text = comp.get_text(strip=True)
                    if text and len(text) > 2 and 'å¹¶å‘ç—‡' not in text:
                        complications.append(text)
        
        return complications
        
    except Exception as e:
        print(f"âŒ å¹¶å‘ç—‡çˆ¬å–å¤±è´¥ {disease_name}: {e}")
        return []
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### 1. åçˆ¬è™«å¤„ç†

```python
def anti_crawler_measures(self):
    # éšæœºå»¶è¿Ÿ
    time.sleep(random.uniform(1, 3))
    
    # éšæœºUser-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
    ]
    self.session.headers.update({'User-Agent': random.choice(user_agents)})
    
    # ä»£ç†è®¾ç½®ï¼ˆå¯é€‰ï¼‰
    # self.session.proxies = {'http': 'proxy_url', 'https': 'proxy_url'}
```

### 2. æ•°æ®éªŒè¯

```python
def validate_data(self, disease_data):
    required_fields = ['name', 'description']
    
    for field in required_fields:
        if not disease_data.get(field):
            print(f"âš ï¸ æ•°æ®ä¸å®Œæ•´: {disease_data['name']} ç¼ºå°‘ {field}")
            return False
    
    return True
```

### 3. é”™è¯¯å¤„ç†

```python
def handle_errors(self, error, disease_name):
    if "Connection timeout" in str(error):
        print(f"â° è¿æ¥è¶…æ—¶: {disease_name}")
        time.sleep(5)  # ç­‰å¾…åé‡è¯•
    elif "404" in str(error):
        print(f"âŒ é¡µé¢ä¸å­˜åœ¨: {disease_name}")
    elif "403" in str(error):
        print(f"ğŸš« è®¿é—®è¢«æ‹’ç»: {disease_name}")
        time.sleep(10)  # ç­‰å¾…æ›´é•¿æ—¶é—´
    else:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {disease_name} - {error}")
```

## ğŸ“Š æ•°æ®è´¨é‡ä¿è¯

### 1. æ•°æ®æ¸…æ´—è§„åˆ™

```python
def clean_text(self, text):
    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    
    # å»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
    
    # å»é™¤é‡å¤å†…å®¹
    text = re.sub(r'(.)\1{3,}', r'\1\1\1', text)
    
    return text
```

### 2. æ•°æ®å»é‡

```python
def remove_duplicates(self, data_list):
    seen = set()
    unique_data = []
    
    for item in data_list:
        if item not in seen:
            seen.add(item)
            unique_data.append(item)
    
    return unique_data
```

### 3. æ•°æ®éªŒè¯

```python
def validate_crawled_data(self, disease_data):
    validation_results = {
        'is_valid': True,
        'issues': []
    }
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    if not disease_data.get('name'):
        validation_results['issues'].append('ç¼ºå°‘ç–¾ç—…åç§°')
        validation_results['is_valid'] = False
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    if len(disease_data.get('description', '')) < 10:
        validation_results['issues'].append('ç–¾ç—…æè¿°è¿‡çŸ­')
    
    if len(disease_data.get('symptoms', [])) == 0:
        validation_results['issues'].append('ç¼ºå°‘ç—‡çŠ¶ä¿¡æ¯')
    
    return validation_results
```

## ğŸ¯ ä½¿ç”¨å»ºè®®

### 1. è¿è¡Œå‰å‡†å¤‡
- ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š
- æ£€æŸ¥ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®
- å‡†å¤‡è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´

### 2. è¿è¡Œä¸­ç›‘æ§
- è§‚å¯Ÿçˆ¬å–è¿›åº¦
- æ£€æŸ¥æ•°æ®è´¨é‡
- å¤„ç†é”™è¯¯æƒ…å†µ

### 3. è¿è¡Œåå¤„ç†
- éªŒè¯æ•°æ®å®Œæ•´æ€§
- æ¸…æ´—å’Œæ ¼å¼åŒ–æ•°æ®
- å¤‡ä»½é‡è¦æ•°æ®

## ğŸ“ˆ é¢„æœŸç»“æœ

è¿è¡Œå®Œæˆåï¼Œæ‚¨å°†è·å¾—ï¼š

1. **MongoDBæ•°æ®åº“**ï¼šç»“æ„åŒ–çš„åŒ»ç–—æ•°æ®
2. **JSONæ–‡ä»¶**ï¼šä¾¿äºå¤„ç†çš„æ•°æ®æ ¼å¼
3. **æ•°æ®ç»Ÿè®¡**ï¼šçˆ¬å–æ•°æ®çš„è¯¦ç»†ç»Ÿè®¡
4. **è´¨é‡æŠ¥å‘Š**ï¼šæ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **éµå®ˆrobots.txt**ï¼šå°Šé‡ç½‘ç«™çš„çˆ¬è™«è§„åˆ™
2. **åˆç†å»¶è¿Ÿ**ï¼šé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
3. **æ•°æ®ä½¿ç”¨**ï¼šä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„
4. **æ³•å¾‹åˆè§„**ï¼šç¡®ä¿æ•°æ®ä½¿ç”¨ç¬¦åˆç›¸å…³æ³•å¾‹æ³•è§„


