# 医疗数据爬取详细指南

## 🎯 爬取目标

### 数据来源
- **网站**：寻医问药网 (http://jib.xywy.com/)
- **数据类型**：疾病信息、症状、病因、治疗、并发症等
- **存储方式**：MongoDB + JSON文件

## 🚀 快速开始

### 1. 安装依赖包
```bash
pip3 install requests beautifulsoup4 pymongo
```

### 2. 启动MongoDB（可选）
```bash
# 如果使用MongoDB存储
mongod --dbpath /path/to/your/db
```

### 3. 运行爬虫
```bash
python3 完整数据爬取系统.py
```

## 📊 爬取数据详情

### 1. 疾病信息爬取

#### 目标页面结构
```
http://jib.xywy.com/il_sii_gaishu/{疾病名称}.htm
```

#### 爬取字段
- **疾病名称**：疾病的中文名称
- **疾病描述**：疾病的详细描述
- **症状信息**：疾病的主要症状
- **病因分析**：疾病的原因分析
- **预防措施**：疾病的预防方法
- **治疗方法**：疾病的治疗方式
- **饮食建议**：相关的饮食指导
- **并发症**：疾病的并发症信息

### 2. 并发症信息爬取

#### 目标页面结构
```
http://jib.xywy.com/il_sii_gaishu/{疾病名称}_bingfa.htm
```

#### 爬取内容
- **并发症名称**：并发症的中文名称
- **并发症描述**：并发症的详细描述
- **关联疾病**：与原疾病的关系
- **严重程度**：并发症的严重程度

## 🔧 爬虫技术实现

### 1. 基础爬虫类

```python
class MedicalDataSpider:
    def __init__(self):
        self.base_url = "http://jib.xywy.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
```

### 2. 数据提取方法

#### HTML解析
```python
def parse_disease_page(self, html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 提取疾病描述
    desc_elements = soup.find_all(['p', 'div'], class_=re.compile(r'desc|intro'))
    
    # 提取症状信息
    symptom_elements = soup.find_all(['li', 'p'], text=re.compile(r'症状'))
    
    # 提取病因信息
    cause_elements = soup.find_all(['li', 'p'], text=re.compile(r'病因'))
    
    return extracted_data
```

#### 数据清洗
```python
def clean_data(self, raw_data):
    # 去除HTML标签
    cleaned_text = re.sub(r'<[^>]+>', '', raw_data)
    
    # 去除多余空白
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    
    # 去除特殊字符
    cleaned_text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', cleaned_text)
    
    return cleaned_text
```

### 3. 数据存储

#### MongoDB存储
```python
def save_to_mongodb(self, disease_data):
    try:
        client = MongoClient('localhost', 27017)
        db = client['medical_qa']
        
        # 保存疾病信息
        diseases_collection = db['diseases']
        diseases_collection.insert_one(disease_data)
        
        # 保存并发症信息
        if disease_data['complications']:
            complications_collection = db['complications']
            for complication in disease_data['complications']:
                comp_data = {
                    'disease_name': disease_data['name'],
                    'complication_name': complication,
                    'crawl_time': disease_data['crawl_time']
                }
                complications_collection.insert_one(comp_data)
        
        print(f"✅ 保存到MongoDB: {disease_data['name']}")
        return True
        
    except Exception as e:
        print(f"❌ MongoDB保存失败: {e}")
        return False
```

#### JSON文件存储
```python
def export_to_json(self):
    try:
        # 从MongoDB读取数据
        diseases = list(self.db['diseases'].find({}, {'_id': 0}))
        complications = list(self.db['complications'].find({}, {'_id': 0}))
        
        # 组织数据
        output_data = {
            'diseases': diseases,
            'complications': complications,
            'export_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_diseases': len(diseases),
            'total_complications': len(complications)
        }
        
        # 保存到JSON文件
        with open('crawled_medical_data.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"✅ 导出完成: {len(diseases)} 个疾病, {len(complications)} 个并发症")
        
    except Exception as e:
        print(f"❌ JSON导出失败: {e}")
```

## 📋 爬取流程详解

### 1. 数据获取流程

```
开始 → 获取疾病列表 → 遍历疾病 → 爬取疾病信息 → 爬取并发症 → 数据清洗 → 数据存储 → 导出JSON → 结束
```

### 2. 详细步骤

#### 步骤1：获取疾病列表
```python
def get_disease_list(self):
    # 从预定义列表或网站获取疾病名称
    disease_names = [
        "高血压", "糖尿病", "冠心病", "脑梗塞", "肺炎", "胃炎", 
        "肝炎", "肾炎", "哮喘", "肺结核", "肺癌", "乳腺癌"
    ]
    
    disease_urls = []
    for disease in disease_names:
        url = f"{self.base_url}/il_sii_gaishu/{disease}.htm"
        disease_urls.append({'name': disease, 'url': url})
    
    return disease_urls
```

#### 步骤2：爬取单个疾病
```python
def crawl_disease_info(self, disease_info):
    try:
        # 发送HTTP请求
        response = self.session.get(disease_info['url'], timeout=10)
        response.encoding = 'utf-8'
        
        # 解析HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 提取数据
        disease_data = self.extract_disease_data(soup, disease_info)
        
        return disease_data
        
    except Exception as e:
        print(f"❌ 爬取失败 {disease_info['name']}: {e}")
        return None
```

#### 步骤3：数据提取
```python
def extract_disease_data(self, soup, disease_info):
    disease_data = {
        'name': disease_info['name'],
        'url': disease_info['url'],
        'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'description': '',
        'symptoms': [],
        'causes': [],
        'prevention': [],
        'treatment': [],
        'diet': [],
        'complications': []
    }
    
    # 提取疾病描述
    desc_elements = soup.find_all(['p', 'div'], class_=re.compile(r'desc|intro|summary'))
    if desc_elements:
        disease_data['description'] = desc_elements[0].get_text(strip=True)
    
    # 提取症状信息
    symptom_elements = soup.find_all(['li', 'p'], text=re.compile(r'症状|表现'))
    for element in symptom_elements:
        parent = element.find_parent()
        if parent:
            symptoms = parent.find_all(['li', 'span'])
            for symptom in symptoms:
                text = symptom.get_text(strip=True)
                if text and len(text) > 2:
                    disease_data['symptoms'].append(text)
    
    # 提取其他信息...
    
    return disease_data
```

#### 步骤4：并发症爬取
```python
def crawl_complications(self, disease_name):
    try:
        # 构造并发症页面URL
        complication_url = f"{self.base_url}/il_sii_gaishu/{disease_name}_bingfa.htm"
        
        response = self.session.get(complication_url, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        complications = []
        
        # 提取并发症信息
        comp_elements = soup.find_all(['li', 'p', 'div'], text=re.compile(r'并发症|合并症'))
        for element in comp_elements:
            parent = element.find_parent()
            if parent:
                comps = parent.find_all(['li', 'span', 'a'])
                for comp in comps:
                    text = comp.get_text(strip=True)
                    if text and len(text) > 2 and '并发症' not in text:
                        complications.append(text)
        
        return complications
        
    except Exception as e:
        print(f"❌ 并发症爬取失败 {disease_name}: {e}")
        return []
```

## 🛠️ 高级功能

### 1. 反爬虫处理

```python
def anti_crawler_measures(self):
    # 随机延迟
    time.sleep(random.uniform(1, 3))
    
    # 随机User-Agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
    ]
    self.session.headers.update({'User-Agent': random.choice(user_agents)})
    
    # 代理设置（可选）
    # self.session.proxies = {'http': 'proxy_url', 'https': 'proxy_url'}
```

### 2. 数据验证

```python
def validate_data(self, disease_data):
    required_fields = ['name', 'description']
    
    for field in required_fields:
        if not disease_data.get(field):
            print(f"⚠️ 数据不完整: {disease_data['name']} 缺少 {field}")
            return False
    
    return True
```

### 3. 错误处理

```python
def handle_errors(self, error, disease_name):
    if "Connection timeout" in str(error):
        print(f"⏰ 连接超时: {disease_name}")
        time.sleep(5)  # 等待后重试
    elif "404" in str(error):
        print(f"❌ 页面不存在: {disease_name}")
    elif "403" in str(error):
        print(f"🚫 访问被拒绝: {disease_name}")
        time.sleep(10)  # 等待更长时间
    else:
        print(f"❌ 未知错误: {disease_name} - {error}")
```

## 📊 数据质量保证

### 1. 数据清洗规则

```python
def clean_text(self, text):
    # 去除HTML标签
    text = re.sub(r'<[^>]+>', '', text)
    
    # 去除多余空白
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 去除特殊字符
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
    
    # 去除重复内容
    text = re.sub(r'(.)\1{3,}', r'\1\1\1', text)
    
    return text
```

### 2. 数据去重

```python
def remove_duplicates(self, data_list):
    seen = set()
    unique_data = []
    
    for item in data_list:
        if item not in seen:
            seen.add(item)
            unique_data.append(item)
    
    return unique_data
```

### 3. 数据验证

```python
def validate_crawled_data(self, disease_data):
    validation_results = {
        'is_valid': True,
        'issues': []
    }
    
    # 检查必需字段
    if not disease_data.get('name'):
        validation_results['issues'].append('缺少疾病名称')
        validation_results['is_valid'] = False
    
    # 检查数据质量
    if len(disease_data.get('description', '')) < 10:
        validation_results['issues'].append('疾病描述过短')
    
    if len(disease_data.get('symptoms', [])) == 0:
        validation_results['issues'].append('缺少症状信息')
    
    return validation_results
```

## 🎯 使用建议

### 1. 运行前准备
- 确保网络连接稳定
- 检查目标网站是否可访问
- 准备足够的存储空间

### 2. 运行中监控
- 观察爬取进度
- 检查数据质量
- 处理错误情况

### 3. 运行后处理
- 验证数据完整性
- 清洗和格式化数据
- 备份重要数据

## 📈 预期结果

运行完成后，您将获得：

1. **MongoDB数据库**：结构化的医疗数据
2. **JSON文件**：便于处理的数据格式
3. **数据统计**：爬取数据的详细统计
4. **质量报告**：数据质量评估报告

## ⚠️ 注意事项

1. **遵守robots.txt**：尊重网站的爬虫规则
2. **合理延迟**：避免对服务器造成过大压力
3. **数据使用**：仅用于学习和研究目的
4. **法律合规**：确保数据使用符合相关法律法规
